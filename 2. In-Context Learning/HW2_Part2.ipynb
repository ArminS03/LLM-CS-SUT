{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viBM9_mkFWKo"
      },
      "source": [
        "# Homework 2 Part 2\n",
        "\n",
        "## Course Name: Large Language Models\n",
        "#### Lecturers: Dr. Soleimani, Dr. Rohban, Dr. Asgari\n",
        "\n",
        "---\n",
        "\n",
        "#### Notebooks Supervised By: MohammadAli SadraeiJavaheri, Omid Ghahroodi\n",
        "#### Notebook Prepared By: Mahdi Zakizadeh, Ali Razghandi\n",
        "\n",
        "**Contact**: Ask your questions in Quera\n",
        "\n",
        "---\n",
        "\n",
        "### Instructions:\n",
        "- Complete all exercises presented in this notebook.\n",
        "- Ensure you run each cell after you've entered your solution.\n",
        "- After completing the exercises, save the notebook and <font color='red'>follow the submission guidelines provided in the PDF.</font>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: Replace the placeholders (between <font color=\"green\">`## Your code begins ##`</font> and <font color=\"green\">`## Your code ends ##`</font>) with the appropriate details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwMRx9vOF2H4"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "The advent of Large Language Models (LLMs) has undeniably shifted the paradigm in the realm of natural language processing, offering capabilities that inch closer to human-like text understanding and generation. Among the vanguards of this shift is the Llama-2 model, a behemoth trained on diverse text corpora, promising adeptness in various NLP tasks. However, as we usher into this era of seemingly intelligent machines, a pertinent question arises - do these models truly understand the text, or do they merely excel in retrieving memorized pieces of information from their training data? This inquiry is not merely academic; the implications of the findings reverberate through the practical applications and the future trajectory of LLMs. In exploring the reasoning capabilities of Large Language Models, a noteworthy investigation was carried out by [Saparov and He](https://openreview.net/pdf?id=qFVVBzXxR2V). Their analytical journey led to the revelation that these models, to a significant extent, harness the knowledge acquired during the pre-training phase when confronted with reasoning tasks. Characterized as \"greedy reasoners,\" these models exhibit a propensity to rely on the reservoir of memorized information, as opposed to showcasing authentic reasoning abilities.\n",
        "\n",
        "Our exploration is set against the backdrop of the SQuAD dataset, a well-regarded benchmark in the question-answering domain. The choice of SQuAD is motivated by its structured evaluation metrics which offer a tangible measure of a model's ability to retrieve and reason over text. While SQuAD has been instrumental in driving progress in question answering, its conventional usage may not fully expose the nuanced capabilities of models like Llama-2. This homework aims to delve deeper by constructing adversarial datasets that challenge the model beyond mere retrieval, probing its ability to reason and refer to the provided context accurately. Through a systematic evaluation on both the original and adversarially-modified versions of the SQuAD dataset, we aspire to dissect the retrieval and reasoning prowess of Llama-2, shedding light on the model's strengths, weaknesses, and the path towards more robust and interpretable LLMs.\n",
        "\n",
        "Let's begin by setting up our workspace and loading the Llama-2 model to explore its capabilities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "4jBgrqILlaRM"
      },
      "outputs": [],
      "source": [
        "# @title Environment Setup\n",
        "# Note: Do NOT make changes to this block.\n",
        "# ----------------\n",
        "%pip install ctransformers[cuda]>=0.2.24 transformers datasets\n",
        "!apt-get -y install -qq aria2\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import random\n",
        "import spacy\n",
        "import transformers\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "SEED=21\n",
        "\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf -d /content/models -o llama-2-7b-chat.Q5_K_M.gguf\n",
        "\n",
        "clear_output()\n",
        "# ----------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "SvdKtpt2myRo"
      },
      "outputs": [],
      "source": [
        "# @title Model Initialization\n",
        "gpu_layers = 200000\n",
        "context_length = 2048\n",
        "\n",
        "from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    \"TheBloke/Llama-2-13B-GGUF\",\n",
        "    model_file=\"/content/models/llama-2-7b-chat.Q5_K_M.gguf\",\n",
        "    model_type=\"llama\",\n",
        "    gpu_layers=gpu_layers,\n",
        "    context_length=context_length,\n",
        ")\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "[INST] <<SYS>\n",
        "%s\n",
        "<</SYS>>\n",
        "%s[/INST]\n",
        "\"\"\"\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX43KiPVKOyg"
      },
      "source": [
        "Now, it's a good practice to test the model with some inputs to get a feel for its responses before diving into the core analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qKAIyhopCBf5",
        "outputId": "df77adb4-247b-439b-affe-e1b23e3ab27a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Not enough info.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title Let's Test the Model\n",
        "preprompt = \"Your job is to answer the users' question accurately according to the context in shortest way possible. If the answer is not present in the provided context by the user, refuse to answer and yield \\\\\\\"Not enough info.\\\\\\\" If the answer is present in the context, only return the part of the context relevant to the question. The shorter your answer be, the more score you receive, even if you write a one word instead of a full sentence. Answering based on your prior knowledge is not considered as a good thing.\" # @param {type:\"string\"}\n",
        "test_input = \"Who is the current president of Iran?\" # @param {type:\"string\"}\n",
        "\n",
        "llm(prompt_template % (preprompt, test_input))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk_ztg5fPjrd"
      },
      "source": [
        "### 2.1. Metrics\n",
        "\n",
        "Evaluating the performance of Large Language Models (LLMs) on question-answering tasks necessitates employing metrics that accurately reflect the models' ability to provide correct and precise answers. Two widely acknowledged metrics for this purpose are Exact Match (EM) and F1 Score, which offer a lens through which the accuracy and the overall quality of the modelâ€™s responses can be gauged.\n",
        "\n",
        "1. **Exact Match (EM)**:\n",
        "   - The Exact Match metric measures the percentage of responses that match the ground truth answers exactly. It is a stringent metric that requires the predicted answer to be identical to the ground truth answer.\n",
        "   - Mathematical Equation:\n",
        "$\\text{EM} = \\left( \\frac{\\text{Number of exact matches}}{\\text{Total number of questions}} \\right) \\times 100$\n",
        "\n",
        "   - Example:\n",
        "     Suppose we have $5$ questions, and the model answers $3$ of them exactly as in the ground truth. The EM score would be $(3/5) \\times 100 = 60 \\\\% $.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tGC7dWSVFM2Q"
      },
      "outputs": [],
      "source": [
        "def compute_exact_match_score(predictions: list[str], ground_truths: list[list[str]]):\n",
        "    exact_match_score = 0\n",
        "    for prediction, ground_truth in zip(predictions, ground_truths):\n",
        "        prediction = prediction.lower().strip()\n",
        "        exact_match_score += any(gt.lower().strip() == prediction for gt in ground_truth)\n",
        "    em_percentage = (exact_match_score / len(predictions)) * 100\n",
        "    return em_percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88dqp5_CbOSG"
      },
      "source": [
        "2. **F1 Score**:\n",
        "   - The F1 Score is the harmonic mean of precision and recall, providing a balance between the two. It measures the overlap between the predicted answers and the ground truth, considering both the words that were correctly included and those that were omitted or added incorrectly.\n",
        "   - Mathematical Equations:\n",
        "   \n",
        "  \\begin{align}\n",
        "  \\text{Precision} = \\frac{\\text{Number of true positive words}}{\\text{(Number of true positive words + Number of false positive words)}}\n",
        "  \\end{align}\n",
        "\n",
        "  \\begin{align}\n",
        "  \\text{Recall} = \\frac{\\text{Number of true positive words}}{\\text{(Number of true positive words + Number of false negative words)}}\n",
        "  \\end{align}\n",
        "\n",
        "  \\begin{align}\n",
        "  \\text{F1 Score} = 2 \\times \\left( \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\right)\n",
        "  \\end{align}\n",
        "  \n",
        "   - Example:\n",
        "     Suppose a predicted answer contains $4$ correct words out of $5$ total words, but misses $2$ words that are in the ground truth answer. The precision would be $4/(4+1) = 0.8$, the recall would be $4/(4+2) = 0.67$, and the F1 Score would be $2 \\times (0.8 \\times 0.67)/(0.8 + 0.67) â‰ˆ 0.73$.\n",
        "\n",
        "These metrics provide a nuanced view of the model's performance, offering insights into not only how often the model is correct (EM), but also how well it captures the nuances of the ground truth answers (F1 Score). Through these metrics, the evaluation phase aims to paint a comprehensive picture of the model's proficiency in the question-answering task amidst the structured framework provided by the SQuAD dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Oa2VDOU8bNos",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def compute_f1_score(predictions: list[str], ground_truths: list[list[str]]):\n",
        "    total_f1_score = 0\n",
        "    for prediction, ground_truth in zip(predictions, ground_truths):\n",
        "        prediction_words = prediction.lower().strip().split()\n",
        "        best_f1 = 0\n",
        "        for gt in ground_truth:\n",
        "            gt_words = gt.lower().strip().split()\n",
        "            common_words_count = sum(1 for word in prediction_words if word in gt_words)\n",
        "            precision = common_words_count / len(prediction_words)\n",
        "            recall = common_words_count / len(gt_words)\n",
        "            f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "            best_f1 = max(best_f1, f1)\n",
        "        total_f1_score += best_f1\n",
        "    average_f1_score = total_f1_score / len(predictions)\n",
        "    return average_f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x-awjXFDNKA"
      },
      "source": [
        "### 2.2. Loading the Dataset and Evaluating the Model\n",
        "\n",
        "Now, let's put the model to the test on the vanilla dataset to see how it performs. The steps we are going to follow are quite straightforward: First, we'll load up the dataset, and then we'll feed it to the model and evaluate the results using the score functions you've implemented earlier. To keep things manageable and ensure a quick run time, we'll use a subset of the SQuAD dataset for this evaluation.\n",
        "\n",
        "In the following step, we'll load a subset of the SQuAD dataset which will be used for evaluating the model. This dataset contains a variety of questions along with the correct answers which we'll compare against the model's responses. After running the code block, you should see a sample row from the dataset, giving you a glimpse of the kind of questions and answers it contains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEaeIR7nM8Qu",
        "outputId": "65c6e1ca-27d5-45e4-dc29-a0440072dd3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '56be4db0acb8001400a502ec',\n",
              " 'title': 'Super_Bowl_50',\n",
              " 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
              " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
              " 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
              "  'answer_start': [177, 177, 177]}}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title Loading the SQuAD Dataset Subset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('squad', split=\"validation\")\n",
        "dataset_test = dataset.shard(num_shards=30, index=0)\n",
        "\n",
        "clear_output()\n",
        "dataset_test[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15yF-T_8BLMU"
      },
      "source": [
        "With the dataset ready, it's time to see how Llama-2 fares. We'll feed the questions from the dataset to the model and collect its answers. Then, we'll use the score functions to calculate the Exact Match and F1 scores for each response, giving us a clear picture of the model's performance on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "750479ceee514187906486f48d36ca1c",
            "fdd69f14541f4d29b8d9f55ffbb1cebf",
            "a0fbecd1831c47a98077dddf73551a43",
            "5fea36927e0f4cb795b3b24dcf181cd6",
            "c7ab77a43c404275989550fd6546a879",
            "65452568ff9c4020bab50679e31f1885",
            "1dbc5a809e8249d9b4a954ba4d1a8ca6",
            "df0f2f3b9b4e442f80c5e69356ac45db",
            "745a6335fbb1498f97eecf806e251693",
            "ab9ef6f28f7e440da92c671f3cd69847",
            "4e90041b44e24ea0b7c027e39cff6b41"
          ]
        },
        "id": "sbZBcUeicCMt",
        "outputId": "b1487d9c-2b4c-41c2-99c6-59ce2a3fc6bf",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "750479ceee514187906486f48d36ca1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/353 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EM Score=(18.69688385269122,), F1 Score=0.37896507808988594\n"
          ]
        }
      ],
      "source": [
        "# @title Evaluating Llama-2 on the Dataset\n",
        "predictions = []\n",
        "ground_truths = []\n",
        "\n",
        "for example in tqdm(dataset_test):\n",
        "    input_text = f\"Question: {example['question']} Context: {example['context']}\"\n",
        "    output_text = llm(prompt_template % (preprompt, input_text))\n",
        "    predictions.append(output_text)\n",
        "    ground_truths.append(example['answers']['text'])\n",
        "\n",
        "em_score = compute_exact_match_score(predictions, ground_truths),\n",
        "f1_score = compute_f1_score(predictions, ground_truths)\n",
        "\n",
        "print(f\"EM Score={em_score}, F1 Score={f1_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlscEL_aBP4F"
      },
      "source": [
        "Having seen how the model performs on the vanilla dataset, letâ€™s delve into some analytical reflections:\n",
        "1. <font color=\"green\"> What do you think is the better metric for evaluating Llama-2 on this dataset and why? </font>\n",
        "2. <font color=\"green\"> How can preprompt text affect the evaluation and the model's performance? </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taIy4S1NNJ2u"
      },
      "source": [
        "<font color=\"red\"> Asnwer 1: </font> <br>\n",
        "\n",
        "The F1 score might be a better metric for evaluating Llama-2 on this dataset. Because there are often many valid ways to express the same idea. The F1 score allows this, rewarding answers that are essentially correct even if they are not word-for-word matches. This metric can provide a better view of the model's understanding and generation capabilities.\n",
        "\n",
        "<font color=\"red\"> Asnwer 2: </font> <br>\n",
        "\n",
        "Preprompt text can frame the answer model generates which is useful for extracting and evaluating the results. Also, it could direct the model to refrain from hallucinating and generating extra information not asked of it. Defining constraint that limit the way the model think is another useful effect of preprompt texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO4K33hL1EC1"
      },
      "source": [
        "## 3. Adversarial Dataset Construction\n",
        "\n",
        "In this section, we venture into the realm of adversarial evaluation to delve deeper into the abilities of the Llama-2 model. The objective is to scrutinize how the model responds to scenarios that are crafted to challenge its reasoning and retrieval capacities. We propose three methods to create adversarial datasets, each aimed at examining different facets of the model's behavior.\n",
        "\n",
        "1. **Answer Absence**: In this method, we modify the SQuAD dataset by crafting questions for which the answers do not exist in the provided context.\n",
        "\n",
        "2. **Entity Substitution**: Here, we substitute entity words in the context with other entities to test whether the model relies on retrieval or refers to the context accurately for answering the question. For instance, changing the context from \"The president of the USA lives in the White House. Barack Obama is the current president of the USA.\" to \"The president of the USA lives in the White House. Gall Granuaile is the current president of the USA.\" and observing if the answer changes appropriately.\n",
        "\n",
        "3. **Nonsense Word Substitution**: In this method, we replace certain words or entities with nonsensical words in a consistent and meaningful way, defining the nonsense words before asking the question. For example, replacing \"White House\" with \"Glibber House\" and explaining that \"Glibber\" means \"White\".\n",
        "\n",
        "Before embarking on the evaluation using adversarial datasets, we encourage students to ponder upon a few analytical questions:\n",
        "<font color=\"green\">\n",
        "\n",
        "3. What is your expectation regarding the model's performance on these adversarial datasets?\n",
        "4. How might the model's behavior on standard versus adversarial datasets inform us about its reasoning and retrieval abilities?\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color=\"red\"> Asnwer 3: </font> <br>\n",
        "\n",
        "Answer Absence: I would except this method to have the worst results as the answers don't exist in the context so it impossible for the model to give correct answers. The model might either fabricate a response or indicate that the answer is not available.\n",
        "\n",
        "Entity Substitution: In this case, the model's performance might vary. If the LLM heavily relies on memorized knowledge, it may not recognize the incorrect or altered context and provide an answer based on its pre-existing knowledge. So the performance of the model might not decrease significantly.\n",
        "\n",
        "Nonsense Word Substitution: This is the most challenging scenario for the model. If the nonsense word is consistently defined and used, a LLM might adapt to this new vocabulary and answer correctly. However, this requires the model to not only understand the context but also to adapt to new information that contradicts its training data.\n",
        "\n",
        "<font color=\"red\"> Asnwer 4: </font> <br>\n",
        "\n",
        "A model's performance on adversarial datasets can highlight whether it truly understands the content or simply retrieves memorized information. Struggles with entity substitution or nonsense word substitution could indicate reliance on memorization over contextual understanding.\n",
        "\n",
        "How a model handles nonsense word substitution can demonstrate its ability to adapt to new information and context, a crucial aspect of human-like understanding and reasoning.\n",
        "\n",
        "Differences in performance between standard and adversarial datasets can help identify specific weaknesses or biases in the model, guiding future improvements and training strategies.\n",
        "\n",
        "The model's ability to handle adversarial scenarios can indicate its robustness and reliability, which are important for practical applications, especially in unpredictable or unconventional contexts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFS9VMsfA8Yl"
      },
      "source": [
        "### 3.1. Answer Absence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Tsz1Ov9NWc"
      },
      "source": [
        "#### Modifying the Dataset\n",
        "\n",
        "For this section we need to modify the original dataset in the way that for each example there will be a new context that is totally different with the original context of the example.\n",
        "\n",
        "To do so, we suggest that you use the title feature in each example and then swap the context between examples that do not have the same title.\n",
        "\n",
        "*  Of course, this is just a suggestion and you can feel free to implement this section as you desire, as long as it meets the required criteria.\n",
        "\n",
        "Some key points:\n",
        "\n",
        "*   The goal is for each example to have a new context that differs from the original.\n",
        "* Using the title of each example is one potential way to pair up examples for swapping contexts.\n",
        "* Feel free to use any approach for generating new contexts as long as they meaningfully differ from the originals.\n",
        "* The modified dataset should meet the specifications and requirements for the assignment.\n",
        "* Be creative in how you modify the contexts - the approach suggested is just one option.\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e68b3b20772c4fd2996f8cce9ced7fa3",
            "7a88f5e141984f6895056e76a92c8393",
            "aaaf9cea39a54abb9060c40774918145",
            "7e3d4dc4db25434eb27a50f1315aeb7d",
            "c632e3975a014f118dcc0a87311ccd4e",
            "48670baa2eb44fe58bcee9a3873d6327",
            "951c4c70c05b4090abcc876cdb4ca6cd",
            "aac8ce12de714bdc8aa534366f3fa4c1",
            "4a7d3f41f158455189457ad29ded4123",
            "20403b51f79e4ab988a76a6c01edff4a",
            "2639dcf470634752909286c39e4c3b38"
          ]
        },
        "id": "sLFp2RNyy0xX",
        "outputId": "0ed0dea7-2832-4dfe-9f2a-c034bfda6729"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e68b3b20772c4fd2996f8cce9ced7fa3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/353 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "original_group_contexts = defaultdict(list)\n",
        "for ex in dataset_test:\n",
        "   original_group_contexts[ex['title']].append(ex['context'])\n",
        "\n",
        "def create_adversarial_example(example):\n",
        "    ## Your code begins ##\n",
        "    indices = list(range(len(dataset_test)))\n",
        "    random.shuffle(indices)\n",
        "    new_context = None\n",
        "    for idx in indices:\n",
        "      if dataset_test[idx]['title'] != example['title']:\n",
        "        new_context = dataset_test[idx]['context']\n",
        "        break\n",
        "    example['new_context'] = new_context\n",
        "    return example\n",
        "    ## Your code ends ##\n",
        "\n",
        "shuffled_context_dataset = dataset_test.map(create_adversarial_example)\n",
        "\n",
        "## Your code begins ##\n",
        "adversial_group_contexts = defaultdict(list)\n",
        "for ex in shuffled_context_dataset:\n",
        "   adversial_group_contexts[ex['title']].append(ex['context'])\n",
        "## Your code ends ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIUavi1OFtz4"
      },
      "outputs": [],
      "source": [
        "for ex in shuffled_context_dataset:\n",
        "    assert(ex['context'] != ex['new_context'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46mXJS1ZA8IC"
      },
      "source": [
        "#### Evaluating Model Performance on Modified Dataset\n",
        "\n",
        "Now we will test the performance of our model on the modified dataset to determine how reliant it is on the original contexts.\n",
        "\n",
        "- Compare the model predictions to the original correct answers.\n",
        "- Calculate evaluation metrics.\n",
        "- Analyze whether there is a significant decrease in model performance on the modified dataset and explain your thoughts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "76434c38765748c7b047019ca6ac59ed",
            "00b342abdf9f4f1ea2020aafe595a607",
            "b7482e057ae347c6a3bbfadda5ccfc0c",
            "2e624bed5d5640cda52072c48ec044f3",
            "15f727d71d8d4bca9a3f58a97c276875",
            "457c471dcad24ebab794943859c5bbd2",
            "8c1860e8c8c148ea8f3f529794f0d84d",
            "1a3fa8f18df0467b8565294056cc39d8",
            "ed4a4a64941b45978a5688bebb76ecbe",
            "8fdae3d8b3694823bbc6ac42c48d2c39",
            "a1c6ebbce2a547178826bec22cfbd8c8"
          ]
        },
        "id": "NYKA4ifmF4-2",
        "outputId": "f53c3370-346f-47b2-f332-5e1d059712bf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76434c38765748c7b047019ca6ac59ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/353 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EM Score=(0.28328611898017,), F1 Score=0.021694415653218217\n"
          ]
        }
      ],
      "source": [
        "###########################################\n",
        "### Evaluating original answers section ###\n",
        "###########################################\n",
        "\n",
        "predictions = []\n",
        "ground_truths = []\n",
        "\n",
        "for example in tqdm(shuffled_context_dataset):\n",
        "    input_text = f\"Context: {example['new_context']} \\nQuestion: {example['question']}\"\n",
        "    output_text = llm(prompt_template % (preprompt, input_text))\n",
        "    predictions.append(output_text)\n",
        "    ground_truths.append(example['answers']['text'])\n",
        "\n",
        "em_score = compute_exact_match_score(predictions, ground_truths),\n",
        "f1_score = compute_f1_score(predictions, ground_truths)\n",
        "\n",
        "print(f\"EM Score={em_score}, F1 Score={f1_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This clearly shows that the absence of proper context can heavily demolish the performance of the model. Which is an obvious result of the lack of information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsZmSH3EDLIn"
      },
      "source": [
        "#### Evaluating \"Not Enough Info.\" Responses\n",
        "\n",
        "In the prompt we specified that the model should respond \"Not enough info.\" if the context lacks the information needed to answer the question.\n",
        "\n",
        "Now we will evaluate the model's performance on these \"not enough info.\" responses.\n",
        "\n",
        "Which evaluation metric should we use and why?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uRvkwjtHZYA"
      },
      "outputs": [],
      "source": [
        "###########################################\n",
        "### Evaluating modified answers section ###\n",
        "###########################################\n",
        "\n",
        "## Your code begins ##\n",
        "\n",
        "## Your code ends ##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I did this part in the cell above it. Considering not enough information is the best output of the model in this dataset, exact match score would not be a good metric for evaluating the model as it would get a very small score eventhough it's generating the best results it can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbPrF4P_XnD4"
      },
      "source": [
        "#### Analyzing Model Responses\n",
        "\n",
        "Now examine some of the model's responses and the corresponding examples to see if anything unusual or interesting occurred during evaluation.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Sample some model responses across the dataset.\n",
        "\n",
        "2. Analyze the input example and model's response.\n",
        "\n",
        "4. Dig deeper into the model's response and explain why this is the case.\n",
        "\n",
        "5. Possible insights:\n",
        "\n",
        "\n",
        "<font color=\"red\"> Asnwer: </font> <br>\n",
        "\n",
        "  - Is model hallucinating or fabricating information? \n",
        "\n",
        "  The model is hallucinating fro  time to time but it seems like that the preprompt text has done a great job of keeping it from fabricating information as it often expresses it doesn't have enough information.\n",
        "\n",
        "  - Does model seem biased or inconsistent?\n",
        "\n",
        "  Model outputs are often \"not enough information\". It seems like the model is biased toward not generating an actual answer. This might be because it is relying too much on the context and isn't using the data learned in pretraining.\n",
        "\n",
        "  - Does the model rely too much on the context?\n",
        "\n",
        "  As asnwered before the model seems to be relying way too much on the context and not using it's previous knowledge as it often claimes it doesn't have enough information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ardC7XmPA_Nz"
      },
      "source": [
        "### 3.2. Entity Substitution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YilQGGNVW40_"
      },
      "source": [
        "#### Modifying Entities in Examples\n",
        "\n",
        "For this section, we need to modify the entities in each example with different entities from the same domain.\n",
        "\n",
        "For example, the sentence \"Joe Biden is the president of the US\" could be changed to \"Akbar is the king of England\".\n",
        "\n",
        "To do this, we recommend using the spaCy library and its named entity recognition (NER) capabilities.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Load the `en_core_web_sm` model in spaCy.\n",
        "\n",
        "2. Identify named entities in each example text.\n",
        "\n",
        "3. Decide which entities could be swapped out.\n",
        "\n",
        "4. Replace entities with new random ones from the same domain.\n",
        "\n",
        "\n",
        "**Of course, this is just a suggestion and you can feel free to implement this section as you desire, as long as it meets the required criteria.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M2dIvUxQGdd",
        "outputId": "76f0d123-8b8e-43a3-f249-5c0ec89c89ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('This', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('sentence', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"This is a sentence.\")\n",
        "print([(w.text, w.pos_) for w in doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ovbqtDgQJsQ"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "ner = nlp.get_pipe(\"ner\")\n",
        "labels = ner.labels\n",
        "\n",
        "for label in labels:\n",
        "    print(label)\n",
        "    print(spacy.explain(label))\n",
        "    print('-------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "c0aVoWYoZHd_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "EVENT\n",
        "Named hurricanes, battles, wars, sports events, etc.\n",
        "-------------------------------\n",
        "FAC\n",
        "Buildings, airports, highways, bridges, etc.\n",
        "-------------------------------\n",
        "GPE\n",
        "Countries, cities, states\n",
        "-------------------------------\n",
        "LANGUAGE\n",
        "Any named language\n",
        "-------------------------------\n",
        "LAW\n",
        "Named documents made into laws.\n",
        "-------------------------------\n",
        "LOC\n",
        "Non-GPE locations, mountain ranges, bodies of water\n",
        "-------------------------------\n",
        "NORP\n",
        "Nationalities or religious or political groups\n",
        "-------------------------------\n",
        "ORG\n",
        "Companies, agencies, institutions, etc.\n",
        "-------------------------------\n",
        "PERSON\n",
        "People, including fictional\n",
        "-------------------------------\n",
        "PRODUCT\n",
        "Objects, vehicles, foods, etc. (not services)\n",
        "-------------------------------\n",
        "WORK_OF_ART\n",
        "Titles of books, songs, etc.\n",
        "-------------------------------\n",
        "'''\n",
        "entities = {\n",
        "    \"EVENT\": [\n",
        "        \"Hurricane Katrina\",\n",
        "        \"Battle of Waterloo\",\n",
        "        \"World War II\",\n",
        "        \"Super Bowl\",\n",
        "        \"Vietnam War\",\n",
        "        \"Hurricane Sandy\",\n",
        "        \"Gulf War\",\n",
        "        \"French Open\",\n",
        "        \"Battle of Gettysburg\",\n",
        "        \"FIFA World Cup\"\n",
        "    ],\n",
        "    \"FAC\": [\n",
        "        \"LaGuardia Airport\",\n",
        "        \"Golden Gate Bridge\",\n",
        "        \"CN Tower\",\n",
        "        \"Heathrow Airport Terminal\",\n",
        "        \"Shanghai Metro\",\n",
        "        \"Hoover Dam\",\n",
        "        \"Burj Khalifa\",\n",
        "        \"Cape Canaveral Space Force Station\",\n",
        "        \"CERN Hadron Collider\",\n",
        "        \"Shanghai Tunnel\"\n",
        "    ],\n",
        "    \"GPE\": [\n",
        "        \"Paris\",\n",
        "        \"France\",\n",
        "        \"Canada\",\n",
        "        \"California\",\n",
        "        \"US\",\n",
        "        \"India\",\n",
        "        \"Mexico\",\n",
        "        \"Germany\",\n",
        "        \"New South Wales\",\n",
        "        \"Australia\",\n",
        "        \"Jakarta\",\n",
        "        \"Indonesia\",\n",
        "        \"Shanghai\",\n",
        "        \"China\",\n",
        "        \"Texas\"\n",
        "    ],\n",
        "    \"LANGUAGE\": [\n",
        "        \"English\",\n",
        "        \"Mandarin Chinese\",\n",
        "        \"Spanish\",\n",
        "        \"Arabic\",\n",
        "        \"Russian\",\n",
        "        \"French\",\n",
        "        \"German\",\n",
        "        \"Japanese\",\n",
        "        \"Hindi\",\n",
        "        \"Portuguese\"\n",
        "    ],\n",
        "    \"LAW\": [\n",
        "        \"United States Constitution\",\n",
        "        \"Magna Carta\",\n",
        "        \"Code of Hammurabi\",\n",
        "        \"Declaration of Independence\",\n",
        "        \"Bill of Rights\",\n",
        "        \"Geneva Conventions\",\n",
        "        \"Universal Declaration of Human Rights\",\n",
        "        \"Treaty of Versailles\",\n",
        "        \"Patient Protection and Affordable Care Act\",\n",
        "        \"Civil Rights Act\"\n",
        "    ],\n",
        "    \"LOC\": [\n",
        "        \"Sahara Desert\",\n",
        "        \"Amazon River\",\n",
        "        \"Mount Everest\",\n",
        "        \"Pacific Ocean\",\n",
        "        \"Hudson River\",\n",
        "        \"Urals Mountains\",\n",
        "        \"Lake Victoria\",\n",
        "        \"Strait of Gibraltar\",\n",
        "        \"Antarctica\",\n",
        "        \"Mariana Trench\"\n",
        "    ],\n",
        "\n",
        "    \"NORP\": [\n",
        "        \"Arabs\",\n",
        "        \"Hispanics\",\n",
        "        \"Kurds\",\n",
        "        \"Tamils\",\n",
        "        \"Hutus\",\n",
        "        \"Pashtuns\",\n",
        "        \"Hmong\",\n",
        "        \"Israelis\",\n",
        "        \"Basques\",\n",
        "        \"Chechens\"\n",
        "    ],\n",
        "    \"ORG\": [\n",
        "        \"United Nations\",\n",
        "        \"Microsoft Corporation\",\n",
        "        \"Mayo Clinic\",\n",
        "        \"Taliban\",\n",
        "        \"NASA\",\n",
        "        \"Starbucks\",\n",
        "        \"FIFA\",\n",
        "        \"Centers for Disease Control and Prevention\",\n",
        "        \"European Union\",\n",
        "        \"Harvard University\"\n",
        "    ],\n",
        "    \"PERSON\": [\n",
        "        \"Barack Obama\",\n",
        "        \"Queen Elizabeth II\",\n",
        "        \"Cristiano Ronaldo\",\n",
        "        \"J.K. Rowling\",\n",
        "        \"Elon Musk\",\n",
        "        \"Taylor Swift\",\n",
        "        \"Donald Trump\",\n",
        "        \"Serena Williams\",\n",
        "        \"Jeff Bezos\",\n",
        "        \"Malala Yousafzai\"\n",
        "    ],\n",
        "    \"PRODUCT\": [\n",
        "        \"iPhone\",\n",
        "        \"Coca-Cola\",\n",
        "        \"Boeing 747\",\n",
        "        \"Harry Potter books\",\n",
        "        \"Lego\",\n",
        "        \"PlayStation 5\",\n",
        "        \"Tesla Model S\",\n",
        "        \"Ikea Billy bookcase\",\n",
        "        \"Honda Civic\",\n",
        "        \"Heinz ketchup\"\n",
        "    ],\n",
        "    \"WORK_OF_ART\": [\n",
        "        \"Mona Lisa\",\n",
        "        \"Hamlet\",\n",
        "        \"The Starry Night\",\n",
        "        \"Thriller\",\n",
        "        \"The Odyssey\",\n",
        "        \"The Divine Comedy\",\n",
        "        \"Pride and Prejudice\",\n",
        "        \"La Gioconda\",\n",
        "        \"Broadway musical Hamilton\",\n",
        "        \"Hey Jude\"\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zp_U4fbc9IM"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def change_example_entities(example):\n",
        "    ## Your code begins ##\n",
        "    context = example['context']\n",
        "    changed_context = context.lower()\n",
        "    for v in entities.values():\n",
        "        for name in v:\n",
        "            if name.lower() in changed_context:\n",
        "                new = random.choice(v).upper()\n",
        "                changed_context.replace(name.lower(), new)\n",
        "\n",
        "    example['changed_context'] = changed_context.lower()\n",
        "    return example\n",
        "    ## Your code ends ##\n",
        "\n",
        "changed_entiy_dataset = dataset_test.map(change_example_entities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDlZvOKQacRU"
      },
      "source": [
        "#### Evaluating Model on Modified Entities\n",
        "\n",
        "Now we will evaluate our model's performance on the dataset with modified entities.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Check model performance on original correct answers.\n",
        "\n",
        "2. Check performance on modified answers.\n",
        "\n",
        "   - Calculate metrics on answers changed to match context.\n",
        "\n",
        "3. Examine some model responses.\n",
        "\n",
        "   - Analyze model behavior on modified examples.\n",
        "\n",
        "   - Explain anything interesting about model responses.\n",
        "\n",
        "**Key Points**\n",
        "\n",
        "- Evaluate on original answers as a baseline.\n",
        "\n",
        "- Also evaluate on modified answers matching context.\n",
        "\n",
        "- Compare metrics - does performance decrease?\n",
        "\n",
        "- Inspect some responses for insightful model behaviors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5ngYPIUeJQ9"
      },
      "outputs": [],
      "source": [
        "# ###########################################\n",
        "# ### Evaluating original answers section ###\n",
        "# ###########################################\n",
        "\n",
        "# ## Your code begins ##\n",
        "# predictions = []\n",
        "# references = []\n",
        "\n",
        "# for example in tqdm(dataset):\n",
        "#     pass\n",
        "\n",
        "# print(f\"EM Score={em_score}, F1 Score={f1_score}\")\n",
        "# ## Your code ends ##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "5ebfb1addc594a6db8760565981c6c67",
            "f6990c2be99447adb00196d501790088",
            "c751f099041f493480d1fba843c0567c",
            "fec2966eacfe4908be8bce211b977a5c",
            "4f6239eec27e42ee8dae1b725cf475d1",
            "8dde9ccbfb624ad3a80780b5265603d7",
            "197c0b2ed8d148b08df7af19472df8ab",
            "7727ac9816714f3d9e1c13bb4af4386b",
            "8261d26b14794230a92abe9045a17326",
            "3ada42fa9b00440aa317f65dc64b4414",
            "44dd5156fe164b4181c3329503668468"
          ]
        },
        "id": "3zTzRINreipj",
        "outputId": "c16405d3-ad27-4de6-f158-b8ce1110384e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ebfb1addc594a6db8760565981c6c67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/353 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EM Score=(26.345609065155806,), F1 Score=0.4710253522960759\n"
          ]
        }
      ],
      "source": [
        "###########################################\n",
        "### Evaluating modified answers section ###\n",
        "###########################################\n",
        "\n",
        "## Your code begins ##\n",
        "predictions = []\n",
        "ground_truths = []\n",
        "\n",
        "for example in tqdm(changed_entiy_dataset):\n",
        "    input_text = f\"Context: {example['changed_context']} \\nQuestion: {example['question']}\"\n",
        "    output_text = llm(prompt_template % (preprompt, input_text))\n",
        "    predictions.append(output_text)\n",
        "    ground_truths.append(example['answers']['text'])\n",
        "\n",
        "em_score = compute_exact_match_score(predictions, ground_truths),\n",
        "f1_score = compute_f1_score(predictions, ground_truths)\n",
        "\n",
        "print(f\"EM Score={em_score}, F1 Score={f1_score}\")\n",
        "## Your code ends ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<font color=\"red\"> Asnwer: </font> <br>\n",
        "\n",
        "After evaluating the dataset, we suprisingly find out that the performance of the model have improved compared to the original dataset. This could happen for a number of reasons:\n",
        "\n",
        "1 - Because the substituted entities are still within the context that the model is familiar with, the model might adapt well to these changes. For example, substituting one well-known entity with another might not significantly disrupt the model's understanding of the context, allowing it to maintain or even improve its performance.\n",
        "\n",
        "2 - In some cases, entity substitution can reduce the complexity of potential answers. This can happen if the substituted entities are less ambiguous or have a more direct association with the question, making it easier for the model to identify the correct answer.\n",
        "\n",
        "3 - If the substituted entities are more widespread or better represented in the modelâ€™s pre-training data, the model might respond to them more accurately. Large language models often perform better with concepts and entities they have encountered more frequently during training.\n",
        "\n",
        "Aside from the reasons mentioned above, It's also possible that the observed improvement is coincidental or due to the dataset. For example, the changes made might align more closely with the model's existing biases or strengths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqtcv1c2BBkR"
      },
      "source": [
        "### 3.3. Nonsense Word Substitution\n",
        "\n",
        "In this segment of the adversarial dataset construction, our primary aim is to assess the model's ability to adapt to new, artificially coined terms and evaluate its reasoning capabilities based on the provided context. We will implement a systematic approach to generate nonsense words, replace identifiable entities in the dataset with these generated words, and provide a definition for each nonsense word. This process encapsulates the essence of exploring how well the model can understand and use newly defined terms to answer questions accurately.\n",
        "\n",
        "The first task at hand is to design a function that generates nonsense words. The goal here is to create a word that doesn't carry any pre-existing meaning. The function `generate_nonsense_word` below is your starting point. Implement the function such that it creates and returns a nonsense word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "itwWWQOmHM8W"
      },
      "outputs": [],
      "source": [
        "# @title Generate Nonsense Words (Your Implementation)\n",
        "import string\n",
        "def generate_nonsense_word():\n",
        "    ## Your code begins ##\n",
        "    length = random.randint(3, 10)\n",
        "\n",
        "    alphabet = string.ascii_lowercase\n",
        "\n",
        "    word = ''.join(random.choices(alphabet, k=length))\n",
        "\n",
        "    return word\n",
        "    ## Your code ends ##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAx1jFH5J7u7"
      },
      "source": [
        "Having devised a mechanism to create nonsense words, we transition into the heart of this sectionâ€”creating the adversarial dataset. We will employ the Spacy library's Named Entity Recognition (NER) system to identify entities within the text. Each identified entity will be replaced by a generated nonsense word, and a definition will be provided for every replacement. The create_adversarial_example function below encapsulates this task. Implement the function, and upon executing it, you will observe a sample example from the adversarial dataset that illustrates the substitutions and definitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f16da219dee74b5784eaa5d348ccb681",
            "ed797eb355e641bb94c19a9b702e503b",
            "39ed65fcbd7f445d928db52bbde947d8",
            "99be650c68ae4f8ab609fd0a01222d29",
            "483d129fcb144c479c71313762dd0014",
            "a9cd9392f30f4b9aba2b74e379d44a6e",
            "fb45ace4e86b4133a0b660339ef4e2ee",
            "9f71d4a21eb14989810d83e440908728",
            "72305ac3730341eb83af14b03c0cf9c1",
            "818584ad3bb34e5f85816e514224616e",
            "7cfc50f7fdd447a19f88b8ca5b6519d6"
          ]
        },
        "id": "-yRe-hNFdHPI",
        "outputId": "019e4285-8947-4e64-e877-e0eed198a255"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '56be4db0acb8001400a502ec',\n",
              " 'title': 'Super_Bowl_50',\n",
              " 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
              " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
              " 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
              "  'answer_start': [177, 177, 177]},\n",
              " 'altered_context': 'super bowl 50 was an american football game to determine the champion of the national football league (nfl) for the 2015 season. the american football conference (afc) champion denver broncos defeated the national football conference (nfc) champion carolina panthers 24â€“10 to earn their third super bowl title. the game was played on february 7, 2016, at levi\\'s stadium in the san francisco bay area at santa clara, california. as this was the 50th super bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each super bowl game with roman numerals (under which the game would have been known as \"super bowl l\"), so that the logo could prominently feature the arabic numerals 50.',\n",
              " 'altered_question': 'which nfl team represented the afc at super bowl 50?',\n",
              " 'definitions': 'usortcok is another word for FIFA World Cup, kmjatgf is another word for Cape Canaveral Space Force Station, wqoxbae is another word for Australia, wcx is another word for Hindi, ohfnhybv is another word for Geneva Conventions, eptoinfx is another word for Mariana Trench, lqxmsyxd is another word for Basques, pmswwr is another word for Harvard University, oxk is another word for Queen Elizabeth II, gpf is another word for iPhone, iansrtto is another word for Mona Lisa'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title Create Adversarial Dataset (Your Implementation)\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def create_adversarial_example(example):\n",
        "    # doc = nlp(example['context'])\n",
        "\n",
        "    ## Your code begins ##\n",
        "    entity_replacements = {\n",
        "    random.choice(li): generate_nonsense_word()\n",
        "    for k, li in entities.items()\n",
        "}\n",
        "\n",
        "    altered_context = example['context'].lower()\n",
        "    altered_question = example['question'].lower()\n",
        "    for entity, nonsense_word in entity_replacements.items():\n",
        "        altered_context = altered_context.replace(entity, nonsense_word)\n",
        "        altered_question = altered_question.replace(entity, nonsense_word)\n",
        "\n",
        "    definitions = {v: k for k, v in entity_replacements.items()}\n",
        "    ## Your code ends ##\n",
        "\n",
        "    return {\n",
        "      'altered_context': altered_context,\n",
        "      'altered_question': altered_question,\n",
        "      'definitions': ', '.join([f'{k} is another word for {v}' for k, v in definitions.items()]),\n",
        "    }\n",
        "\n",
        "adversarial_examples = dataset_test.map(create_adversarial_example)\n",
        "\n",
        "clear_output()\n",
        "adversarial_examples[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vDgWGyNKESc"
      },
      "source": [
        "With the adversarial dataset in place, the stage is set for evaluating the model's performance. We aim to uncover how well the model navigates through the maze of newly introduced terms while clinging to the definitions provided. Implement the evaluation code block below to gauge the model's performance on this adversarial dataset. The insights garnered from this exercise will shed light on the model's ability to adapt to new information and reason based on provided definitions, which is a step closer to understanding the model's reasoning faculties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "8f43b17a19d64aa686776ab1b7270fcb",
            "6defad58b5eb4989ac025f135e0a8bfe",
            "f6b5668fa5ab4564b42adfe94683ed71",
            "eb2545e1b6f6446a83d6852eab0c125e",
            "d8d98677336a464ea251e0f1878d7547",
            "2e2e7a09fec3458bb929336e08d508bf",
            "fda2060b26a048d0ac6ac6bff6e7c612",
            "f48d9d9e835749729952bddda5c320a6",
            "3f0c243c6342479d8cebbe8e187d7f87",
            "1033707230184d6bbc3fedb017ac0191",
            "dfeed408c6a54c9abb74aa5c52d8b72e"
          ]
        },
        "id": "3wgqV1gNJuM0",
        "outputId": "16651d76-d903-4a5b-9e94-08984ca39369"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f43b17a19d64aa686776ab1b7270fcb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/353 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EM Score=(5.94900849858357,), F1 Score=0.29327400488431254\n"
          ]
        }
      ],
      "source": [
        "# @title Evaluating Llama-2 on the Adversarial Dataset\n",
        "predictions = []\n",
        "ground_truths = []\n",
        "\n",
        "for example in tqdm(adversarial_examples):\n",
        "    input_text = f\"Question: {example['altered_question']} Context: {example['altered_context']} Definitions: {example['definitions']}\"\n",
        "    output_text = llm(prompt_template % (preprompt, input_text))\n",
        "    predictions.append(output_text)\n",
        "    ground_truths.append(example['answers']['text'])\n",
        "\n",
        "em_score = compute_exact_match_score(predictions, ground_truths)\n",
        "f1_score = compute_f1_score(predictions, ground_truths)\n",
        "\n",
        "print(f\"EM Score={em_score}, F1 Score={f1_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color=\"red\"> Asnwer: </font> <br>\n",
        "\n",
        "As expected, the results of the model have slightly worsened compared to the original dataset. This could happedn bacause the model is struggling to align the newly defined words with the original ones. These new words are surely absent from the pretraining dataset so it makes sense for the model to not fully understand and associate them with other words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vuDiDRN1QkZ"
      },
      "source": [
        "## 4. Conclusion\n",
        "This exercise navigates through the curious interplay of reasoning and retrieval within Large Language Models, particularly focusing on the Llama-2 model. Through meticulous evaluation and crafting adversarial datasets, we aim to provide a window into the model's behavior, shedding light on its strengths, weaknesses, and its approach to deciphering and responding to questions under varying conditions.\n",
        "\n",
        "Now, reflect upon the model's performance and share your insights:\n",
        "\n",
        "\n",
        "5. <font color=\"green\"> Did the model's performance align with your expectations? </font>\n",
        "6. <font color=\"green\"> How do the adversarial evaluations contribute to our understanding of the model's strengths and weaknesses in terms of reasoning and retrieval? </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color=\"red\"> Asnwer 5: </font> <br>\n",
        "\n",
        "Eventhough I expected parts 1 and 3 to behave the way they did, the results of the second part came as a suprise to me. By substituding words with random words from the same domain I was anticipating to encounter decreased resluts as the model is recieving somewhat false information but the performance imprtoved, much to my surprise.\n",
        "\n",
        "<font color=\"red\"> Asnwer 6: </font> <br>\n",
        "\n",
        "Adversarial evaluations are really helpful for understanding how well a model like Llama-2 can reason and retrieve information. By using tricky tests, like changing parts of a text or asking questions with no clear answers, we can see if the model is just repeating what it knows or if it's actually understanding the context. If a model does well in these tough situations, it shows that it's good at figuring things out and not just relying on what it has learned before. But if it struggles, it might mean the model is mostly remembering things and not really getting the deeper meaning. These kinds of tests are important because they give us a clearer picture of what the model can do and where it needs to get better, especially in understanding and using language.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vFS9VMsfA8Yl"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00b342abdf9f4f1ea2020aafe595a607": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_457c471dcad24ebab794943859c5bbd2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8c1860e8c8c148ea8f3f529794f0d84d",
            "value": "100%"
          }
        },
        "1033707230184d6bbc3fedb017ac0191": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15f727d71d8d4bca9a3f58a97c276875": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "197c0b2ed8d148b08df7af19472df8ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a3fa8f18df0467b8565294056cc39d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dbc5a809e8249d9b4a954ba4d1a8ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20403b51f79e4ab988a76a6c01edff4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2639dcf470634752909286c39e4c3b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e2e7a09fec3458bb929336e08d508bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e624bed5d5640cda52072c48ec044f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fdae3d8b3694823bbc6ac42c48d2c39",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a1c6ebbce2a547178826bec22cfbd8c8",
            "value": " 353/353 [1:02:08&lt;00:00, 11.68s/it]"
          }
        },
        "39ed65fcbd7f445d928db52bbde947d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f71d4a21eb14989810d83e440908728",
            "max": 353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72305ac3730341eb83af14b03c0cf9c1",
            "value": 353
          }
        },
        "3ada42fa9b00440aa317f65dc64b4414": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f0c243c6342479d8cebbe8e187d7f87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44dd5156fe164b4181c3329503668468": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "457c471dcad24ebab794943859c5bbd2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "483d129fcb144c479c71313762dd0014": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48670baa2eb44fe58bcee9a3873d6327": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a7d3f41f158455189457ad29ded4123": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e90041b44e24ea0b7c027e39cff6b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f6239eec27e42ee8dae1b725cf475d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ebfb1addc594a6db8760565981c6c67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6990c2be99447adb00196d501790088",
              "IPY_MODEL_c751f099041f493480d1fba843c0567c",
              "IPY_MODEL_fec2966eacfe4908be8bce211b977a5c"
            ],
            "layout": "IPY_MODEL_4f6239eec27e42ee8dae1b725cf475d1"
          }
        },
        "5fea36927e0f4cb795b3b24dcf181cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab9ef6f28f7e440da92c671f3cd69847",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4e90041b44e24ea0b7c027e39cff6b41",
            "value": " 353/353 [1:05:52&lt;00:00, 10.74s/it]"
          }
        },
        "65452568ff9c4020bab50679e31f1885": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6defad58b5eb4989ac025f135e0a8bfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e2e7a09fec3458bb929336e08d508bf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fda2060b26a048d0ac6ac6bff6e7c612",
            "value": "100%"
          }
        },
        "72305ac3730341eb83af14b03c0cf9c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "745a6335fbb1498f97eecf806e251693": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "750479ceee514187906486f48d36ca1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fdd69f14541f4d29b8d9f55ffbb1cebf",
              "IPY_MODEL_a0fbecd1831c47a98077dddf73551a43",
              "IPY_MODEL_5fea36927e0f4cb795b3b24dcf181cd6"
            ],
            "layout": "IPY_MODEL_c7ab77a43c404275989550fd6546a879"
          }
        },
        "76434c38765748c7b047019ca6ac59ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00b342abdf9f4f1ea2020aafe595a607",
              "IPY_MODEL_b7482e057ae347c6a3bbfadda5ccfc0c",
              "IPY_MODEL_2e624bed5d5640cda52072c48ec044f3"
            ],
            "layout": "IPY_MODEL_15f727d71d8d4bca9a3f58a97c276875"
          }
        },
        "7727ac9816714f3d9e1c13bb4af4386b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a88f5e141984f6895056e76a92c8393": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48670baa2eb44fe58bcee9a3873d6327",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_951c4c70c05b4090abcc876cdb4ca6cd",
            "value": "Map: 100%"
          }
        },
        "7cfc50f7fdd447a19f88b8ca5b6519d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e3d4dc4db25434eb27a50f1315aeb7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20403b51f79e4ab988a76a6c01edff4a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2639dcf470634752909286c39e4c3b38",
            "value": " 353/353 [00:00&lt;00:00, 1103.51 examples/s]"
          }
        },
        "818584ad3bb34e5f85816e514224616e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8261d26b14794230a92abe9045a17326": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c1860e8c8c148ea8f3f529794f0d84d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dde9ccbfb624ad3a80780b5265603d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f43b17a19d64aa686776ab1b7270fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6defad58b5eb4989ac025f135e0a8bfe",
              "IPY_MODEL_f6b5668fa5ab4564b42adfe94683ed71",
              "IPY_MODEL_eb2545e1b6f6446a83d6852eab0c125e"
            ],
            "layout": "IPY_MODEL_d8d98677336a464ea251e0f1878d7547"
          }
        },
        "8fdae3d8b3694823bbc6ac42c48d2c39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "951c4c70c05b4090abcc876cdb4ca6cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99be650c68ae4f8ab609fd0a01222d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_818584ad3bb34e5f85816e514224616e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7cfc50f7fdd447a19f88b8ca5b6519d6",
            "value": " 353/353 [00:00&lt;00:00, 1512.12 examples/s]"
          }
        },
        "9f71d4a21eb14989810d83e440908728": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0fbecd1831c47a98077dddf73551a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df0f2f3b9b4e442f80c5e69356ac45db",
            "max": 353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_745a6335fbb1498f97eecf806e251693",
            "value": 353
          }
        },
        "a1c6ebbce2a547178826bec22cfbd8c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9cd9392f30f4b9aba2b74e379d44a6e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaaf9cea39a54abb9060c40774918145": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aac8ce12de714bdc8aa534366f3fa4c1",
            "max": 353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a7d3f41f158455189457ad29ded4123",
            "value": 353
          }
        },
        "aac8ce12de714bdc8aa534366f3fa4c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab9ef6f28f7e440da92c671f3cd69847": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7482e057ae347c6a3bbfadda5ccfc0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a3fa8f18df0467b8565294056cc39d8",
            "max": 353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed4a4a64941b45978a5688bebb76ecbe",
            "value": 353
          }
        },
        "c632e3975a014f118dcc0a87311ccd4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c751f099041f493480d1fba843c0567c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7727ac9816714f3d9e1c13bb4af4386b",
            "max": 353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8261d26b14794230a92abe9045a17326",
            "value": 353
          }
        },
        "c7ab77a43c404275989550fd6546a879": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8d98677336a464ea251e0f1878d7547": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df0f2f3b9b4e442f80c5e69356ac45db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfeed408c6a54c9abb74aa5c52d8b72e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e68b3b20772c4fd2996f8cce9ced7fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a88f5e141984f6895056e76a92c8393",
              "IPY_MODEL_aaaf9cea39a54abb9060c40774918145",
              "IPY_MODEL_7e3d4dc4db25434eb27a50f1315aeb7d"
            ],
            "layout": "IPY_MODEL_c632e3975a014f118dcc0a87311ccd4e"
          }
        },
        "eb2545e1b6f6446a83d6852eab0c125e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1033707230184d6bbc3fedb017ac0191",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dfeed408c6a54c9abb74aa5c52d8b72e",
            "value": " 353/353 [1:46:27&lt;00:00, 17.53s/it]"
          }
        },
        "ed4a4a64941b45978a5688bebb76ecbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed797eb355e641bb94c19a9b702e503b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9cd9392f30f4b9aba2b74e379d44a6e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fb45ace4e86b4133a0b660339ef4e2ee",
            "value": "Map: 100%"
          }
        },
        "f16da219dee74b5784eaa5d348ccb681": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed797eb355e641bb94c19a9b702e503b",
              "IPY_MODEL_39ed65fcbd7f445d928db52bbde947d8",
              "IPY_MODEL_99be650c68ae4f8ab609fd0a01222d29"
            ],
            "layout": "IPY_MODEL_483d129fcb144c479c71313762dd0014"
          }
        },
        "f48d9d9e835749729952bddda5c320a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6990c2be99447adb00196d501790088": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dde9ccbfb624ad3a80780b5265603d7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_197c0b2ed8d148b08df7af19472df8ab",
            "value": "100%"
          }
        },
        "f6b5668fa5ab4564b42adfe94683ed71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f48d9d9e835749729952bddda5c320a6",
            "max": 353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f0c243c6342479d8cebbe8e187d7f87",
            "value": 353
          }
        },
        "fb45ace4e86b4133a0b660339ef4e2ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fda2060b26a048d0ac6ac6bff6e7c612": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdd69f14541f4d29b8d9f55ffbb1cebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65452568ff9c4020bab50679e31f1885",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1dbc5a809e8249d9b4a954ba4d1a8ca6",
            "value": "100%"
          }
        },
        "fec2966eacfe4908be8bce211b977a5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ada42fa9b00440aa317f65dc64b4414",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_44dd5156fe164b4181c3329503668468",
            "value": " 353/353 [1:05:53&lt;00:00, 10.42s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
