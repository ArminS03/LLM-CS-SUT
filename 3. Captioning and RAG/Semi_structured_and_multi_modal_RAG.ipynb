{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "812a4dbc-fe04-4b84-bdf9-390045e30806"
      },
      "source": [
        "## Semi-Structured RAG For Private Data\n",
        "\n",
        "In this homework assignment, you will be delving into the realm of **Retrieval Augmented Generation (RAG)**.\n",
        "\n",
        "Your objective is to construct a system that leverages retrieval from a **private database** consisting of PDFs. These PDFs encapsulate a rich variety of content, including textual information, images, and tables.\n",
        "\n",
        "The challenge lies in preserving all these components while efficiently extracting relevant data based on a user's input question.\n",
        "\n",
        "- As a first step, you will need to develop mechanisms for extracting text from the PDFs. Also, extract textual embeddings for following comparison with the user's input.\n",
        "- Subsequently, you should implement a process to identify and retrieve the most pertinent information matching a user's query.\n",
        "- Because some input texts are too long, we have to summarize them, and then use the summary of the most similar text to LLM as input.\n",
        "- Then, you will integrate this retrieved information with a Large Language Model (LLM) to generate comprehensive and contextually relevant responses to user queries.\n",
        "- Finally, you will apply this mechanism in a Multimodal approach, where you convert PDF images to clip embeddings and use the input's textual CLIP embeddings to compare with the ground truth's image embeddings and find the most similar image to the input text.\n",
        "- As we are using Unimodal LLMs, we can not give those images to the LLM. Hence, we use image captions to be used in LLM's input.\n",
        "\n",
        "This holistic approach ensures that no valuable information is lost, and the system provides nuanced answers by combining both the knowledge embedded in the PDFs and the capabilities of the LLM.\n",
        "\n",
        "Instruction:\n",
        "\n",
        "<font color='77CC99'>Follow the Green texts and fill out the notebook.</font>\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1kODk16WWrn9DqvaWoEAekHRXup1djGjl' width=\"75%\">"
      ],
      "id": "812a4dbc-fe04-4b84-bdf9-390045e30806"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OrQHF5lRXYT"
      },
      "source": [
        "## Packages"
      ],
      "id": "_OrQHF5lRXYT"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "140580ef-5db0-43cc-a524-9c39e04d4df0"
      },
      "outputs": [],
      "source": [
        "# restart kernel after first instllation\n",
        "%%capture\n",
        "!apt-get install -y poppler-utils\n",
        "!apt-get install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "# for image extraction from pdf\n",
        "! pip install PyMuPDF\n",
        "! pip install Pillow\n",
        "# text embedding\n",
        "! pip install -U sentence-transformers\n",
        "! pip install transformers accelerate bitsandbytes>=0.39.0 -q"
      ],
      "id": "140580ef-5db0-43cc-a524-9c39e04d4df0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw9hZJTohzhO"
      },
      "source": [
        "# 0 - Loading Data"
      ],
      "id": "fw9hZJTohzhO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6cEOI6IL5TL"
      },
      "source": [
        "### 0.1 - Downoading the PDF"
      ],
      "id": "e6cEOI6IL5TL"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWwLy036L-IG",
        "outputId": "10b96d47-33a7-4051-c406-9a75c5ad56fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Dall_E_paper.pdf', <http.client.HTTPMessage at 0x79a9d4a36ce0>)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "# Define the name of the PDF file and then download them\n",
        "file_name = \"Dall_E_paper\"\n",
        "\n",
        "url = \"https://arxiv.org/pdf/2204.06125.pdf\"\n",
        "file_path = f\"{file_name}.pdf\"\n",
        "urllib.request.urlretrieve(url, file_path)"
      ],
      "id": "UWwLy036L-IG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74b56bde-1ba0-4525-a11d-cab02c5659e4"
      },
      "source": [
        "## 0.2 - Extract Images and Texts"
      ],
      "id": "74b56bde-1ba0-4525-a11d-cab02c5659e4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIjR7jkHaYPd"
      },
      "source": [
        "Implement mechanisms to extract images and texts from the downloaded PDFs."
      ],
      "id": "jIjR7jkHaYPd"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__dTxER43ST0",
        "outputId": "03758829-3830-40cc-d1f1-e78d05304538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/pdftotext\n"
          ]
        }
      ],
      "source": [
        "!which pdftotext"
      ],
      "id": "__dTxER43ST0"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQP44Bda4ZmG",
        "outputId": "2aee39e8-1367-4be3-c465-86d2d71d6c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.1.1\n"
          ]
        }
      ],
      "source": [
        "import pytesseract\n",
        "print(pytesseract.get_tesseract_version())"
      ],
      "id": "tQP44Bda4ZmG"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lVZAfm2CdmSB"
      },
      "outputs": [],
      "source": [
        "# Import required dependencies\n",
        "import fitz\n",
        "import os\n",
        "from PIL import Image"
      ],
      "id": "lVZAfm2CdmSB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n9HKjA2an9U"
      },
      "source": [
        "#### Step 0.2.1: Extract and Store Images"
      ],
      "id": "9n9HKjA2an9U"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RkdJijDrd0DR"
      },
      "outputs": [],
      "source": [
        "# Open PDF file\n",
        "pdf_file = fitz.open(file_path)\n",
        "\n",
        "# Calculate number of pages in PDF file\n",
        "page_nums = len(pdf_file)\n",
        "\n",
        "# Create empty list to store images information\n",
        "images_list = []\n",
        "\n",
        "# Extract all images information from each page\n",
        "for page_num in range(page_nums):\n",
        "    page_content = pdf_file[page_num]\n",
        "    images_list.extend(page_content.get_images())"
      ],
      "id": "RkdJijDrd0DR"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZrYARvGQd70n"
      },
      "outputs": [],
      "source": [
        "images_path = \"./images/\"\n",
        "Path(images_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#Save all the extracted images\n",
        "for i, image in enumerate(images_list, start=1):\n",
        "    #Extract the image object number\n",
        "    xref = image[0]\n",
        "    #Extract image\n",
        "    base_image = pdf_file.extract_image(xref)\n",
        "    #Store image bytes\n",
        "    image_bytes = base_image['image']\n",
        "    #Store image extension\n",
        "    image_ext = base_image['ext']\n",
        "    #Generate image file name\n",
        "    image_name = file_name + '_' +str(i) + '.' + image_ext\n",
        "    #Save image\n",
        "    with open(os.path.join(images_path, image_name) , 'wb') as image_file:\n",
        "        image_file.write(image_bytes)\n",
        "        image_file.close()"
      ],
      "id": "ZrYARvGQd70n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAiA72BUdTY5"
      },
      "source": [
        "### Step 0.2.2: Extract and Store Texts From PDF Content"
      ],
      "id": "KAiA72BUdTY5"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured[all-docs]==0.11.2 -q"
      ],
      "metadata": {
        "id": "m2xKaHCUB3eR"
      },
      "id": "m2xKaHCUB3eR",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1_67ZhHhokN",
        "outputId": "ce109e22-0845-4b70-ec1a-8f8acc699655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked']\n",
            "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from lxml import html\n",
        "from pydantic import BaseModel\n",
        "from typing import Any, Optional\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "path='./'\n",
        "\n",
        "# Specify the path to the poppler installation\n",
        "poppler_path = './images/'  # Replace with the path obtained from the previous step\n",
        "\n",
        "# Specify the path to the Tesseract OCR installation\n",
        "tesseract_path = '/usr/bin/tesseract'  # Replace with the path obtained from the previous step\n",
        "\n",
        "\n",
        "# Get elements\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename= \"./\"+\"Dall_E_paper.pdf\",\n",
        "    # Using pdf format to find embedded image blocks\n",
        "    extract_images_in_pdf=True,\n",
        "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
        "    # Titles are any sub-section of the document\n",
        "    infer_table_structure=True,\n",
        "    # Post processing to aggregate text once we have the title\n",
        "    chunking_strategy=\"by_title\",\n",
        "    # Chunking params to aggregate text blocks\n",
        "    # Attempt to create a new chunk 1900 chars\n",
        "    # Attempt to keep chunks > 1000 chars\n",
        "    # Hard max on chunks\n",
        "    max_characters=2000,\n",
        "    new_after_n_chars=1900,\n",
        "    combine_text_under_n_chars=1000,\n",
        "    image_output_dir_path=poppler_path,\n",
        "    tesseract_path=tesseract_path,\n",
        ")"
      ],
      "id": "u1_67ZhHhokN"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5f660305-e165-4b6c-ada3-a67a422defb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f7bcfb4-e059-4799-b1fd-1e3456dfc0aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n"
          ]
        }
      ],
      "source": [
        "# Text\n",
        "text_elements = []\n",
        "for element in raw_pdf_elements:\n",
        "    if \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
        "        text_elements.append(str(element))\n",
        "\n",
        "print(len(text_elements))"
      ],
      "id": "5f660305-e165-4b6c-ada3-a67a422defb5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVtv83Y-GXp1"
      },
      "source": [
        "Because some texts are too long, we have to summarize them at first"
      ],
      "id": "nVtv83Y-GXp1"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BVVjUuStGWXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6096ff8b-f276-47dc-dcf1-7c5c7287c920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 100, but your input_length is only 37. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n",
            "Your max_length is set to 100, but your input_length is only 50. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "summarized_text_elements = summarizer(text_elements , max_length=100, do_sample=False)"
      ],
      "id": "BVVjUuStGWXd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0-Vp95ziTp3"
      },
      "source": [
        "# 1 - Unimodal RAG"
      ],
      "id": "t0-Vp95ziTp3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVLZs4eKifj-"
      },
      "source": [
        "## 1.1 - Loading True Text Data as Embeded Vectors"
      ],
      "id": "oVLZs4eKifj-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Fxk_U3jPoS"
      },
      "source": [
        "In this section, we should convert the text data into embedding vectors and store them. Hence, in the following step. having an input, by comparing we can find out the most similar fact with the input.\n",
        "\n",
        "We use this model for [Text-Embedding](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2)"
      ],
      "id": "W_Fxk_U3jPoS"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iFpBR_k3jtvL"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "text_emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "id": "iFpBR_k3jtvL"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lRpeenvCmTqy"
      },
      "outputs": [],
      "source": [
        "text_embeddings = text_emb_model.encode(text_elements, convert_to_tensor=True)"
      ],
      "id": "lRpeenvCmTqy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEG_md7MmxYC"
      },
      "source": [
        "Now, we have all our crucial embeddings. Thus, if we have a new input, we know that we should compare the input's embeddings with the text_embeddings element and find the closest one."
      ],
      "id": "BEG_md7MmxYC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsKmRTsqnIi-"
      },
      "source": [
        "## 1.2 - Unimodal Semi-Structured RAG"
      ],
      "id": "TsKmRTsqnIi-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq1Ua_TNo9US"
      },
      "source": [
        "### Step 1.2.1: Most Similar Ground Truth Text Ectraction"
      ],
      "id": "Iq1Ua_TNo9US"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJdhgBzinObQ"
      },
      "source": [
        "At the fist step, for any given input, we have to have evaluation functions to find the closest embedding vector to the input vectors. We use the Cosine similarity for this operation.\n",
        "\n",
        "<font color='77CC99'>Write a function \"text_embedding_similarity\" to convert input texts to embedded vector and then returns the similarity between the input text and any of the ground truth texts.</font>\n"
      ],
      "id": "JJdhgBzinObQ"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bSMJ8d40nOMK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_similarity(embeddings_1, embeddings_2):\n",
        "\n",
        "  embeddings_1 = embeddings_1 / embeddings_1.norm(dim=-1, keepdim=True)\n",
        "  embeddings_2 = embeddings_2 / embeddings_2.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  return embeddings_1.cpu().detach().numpy() @ embeddings_2.cpu().detach().numpy().T"
      ],
      "id": "bSMJ8d40nOMK"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZOO-eSuSnOHe"
      },
      "outputs": [],
      "source": [
        "def text_embedding_similarity(input_text, text_embeddings, text_emb_model):\n",
        "\n",
        "    ### To Do ###\n",
        "\n",
        "    input_text_emb = text_emb_model.encode(input_text, convert_to_tensor=True)\n",
        "\n",
        "    ### End ###\n",
        "\n",
        "    return get_similarity(text_embeddings, input_text_emb)"
      ],
      "id": "ZOO-eSuSnOHe"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pmI8uPzTnOEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99bc77f4-4c20-435e-b53e-2423e9e04ed2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.18537423, 0.20288187, 0.21293396, 0.20692343, 0.2686563 ,\n",
              "       0.31146652, 0.01873749, 0.21311942, 0.20436674, 0.25198764,\n",
              "       0.26972088, 0.24603592, 0.2582291 , 0.2539547 , 0.23482765,\n",
              "       0.23657551, 0.17059211, 0.15917057, 0.15459305, 0.14453974,\n",
              "       0.11232636, 0.18208514, 0.12710598, 0.37942076, 0.05238129,\n",
              "       0.3243861 , 0.30019337, 0.0696483 , 0.10325827, 0.13214463,\n",
              "       0.02927516, 0.15812725, 0.10174509, 0.26262775, 0.15325922,\n",
              "       0.00922206, 0.31082875, 0.04998646, 0.12929475], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "input_text = \"is DALL-E2 uses a clip model inside?\"\n",
        "text_embedding_similarity(input_text, text_embeddings, text_emb_model)"
      ],
      "id": "pmI8uPzTnOEy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOH0JjIUt6gu"
      },
      "source": [
        "<font color='77CC99'> Now, write a function that finds \"Summaries\" of the k most similar ground truth texts to the user's input. function \"text_retrival\"</font>"
      ],
      "id": "YOH0JjIUt6gu"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UzXXtOLyt3ep"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "\n",
        "def text_retrival(k, input_text, text_embeddings, text_elements, summarized_text_elements, text_emb_model):\n",
        "\n",
        "    ### To Do ###\n",
        "    scores = text_embedding_similarity(input_text, text_embeddings, text_emb_model)\n",
        "    ind = np.argsort(scores)[::-1][:k]\n",
        "    selected_text_elements = np.array(summarized_text_elements)[ind]\n",
        "    ### End ###\n",
        "\n",
        "    return {\"selected_text_elements\": selected_text_elements}"
      ],
      "id": "UzXXtOLyt3ep"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Arn28x_hypVi"
      },
      "source": [
        "### Step 1.2.2: Load the core LLM and Combine them all"
      ],
      "id": "Arn28x_hypVi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJPD8Y91yxs5"
      },
      "source": [
        "We use a Question-answering model as the core of our system. In fact, having the input text and finding the closest ground truth fact to the input text, we can give them both to an LLM to answer the question.\n",
        "\n",
        "Here we load the core LLM for our Unimodal  Semi-Structured RAG. [Model in HF](https://huggingface.co/samwit/koala-7b)"
      ],
      "id": "zJPD8Y91yxs5"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "h5S1lINLzliK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "fcaf954ee4464893ac2f8070d29e8439",
            "99fd43a0605943cfa67dd1bc353ffe91",
            "24a6dfd243624575b76fb91a3e9749a4",
            "d895d677f2f846db8d7570b48115a320",
            "e76ae40090884615a964c7ca40593c44",
            "573db3f4dd374b59bf7a4c24aba5f569",
            "4d15859d1dea4c199cb9c768f034128e",
            "df76991414e24a4bac7a8101d45586e4",
            "9d4fbf7741f84846b027f8e45ce1e2df",
            "84902c91fb87451b89efc9fd16152053",
            "9b8cf41e469142b0b8afa718f16fddf3"
          ]
        },
        "outputId": "cec360e0-3300-4ebb-f2c3-7de562ac2bbc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcaf954ee4464893ac2f8070d29e8439"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
        "import torch\n",
        "import textwrap\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    \"samwit/koala-7b\",\n",
        "    load_in_8bit=True,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"samwit/koala-7b\")"
      ],
      "id": "h5S1lINLzliK"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gPLjb1gmAqkH"
      },
      "outputs": [],
      "source": [
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15)\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "id": "gPLjb1gmAqkH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO2fptLQ3HlW"
      },
      "source": [
        "Now, what follows is our Prompt, based on that, do the task bellow.\n",
        "\n",
        "\n",
        "<font color='77CC99'> Based on the prompt and what we have done before, write a function that answers the user's question by finding the most related ground truth text(fact) by giving the prompt to LLM. Function \"Unimodal_Question_Answering\" </font>"
      ],
      "id": "RO2fptLQ3HlW"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Ij8m2NMZ18DB"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "prompt_text = \"\"\"ANSWER the QUESTION in conformity to on FACTS. \\n\n",
        "FACTS: \\n {text_facts}. \\n\n",
        "QUESTION: {user_question} \\n\n",
        "ANSWER:  \"\"\""
      ],
      "id": "Ij8m2NMZ18DB"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cgH4EVCa3wh7"
      },
      "outputs": [],
      "source": [
        "def Unimodal_Question_Answering(input_text,k=1):\n",
        "\n",
        "    ### To Do ###\n",
        "    prompt_text = \"\"\"ANSWER the QUESTION in conformity to on FACTS. \\n\n",
        "    FACTS: \\n {text_facts}. \\n\n",
        "    QUESTION: {user_question} \\n\n",
        "    ANSWER:  \"\"\"\n",
        "    facts = ''\n",
        "    summaries = text_retrival(k, input_text, text_embeddings, text_elements, summarized_text_elements, text_emb_model)['selected_text_elements'].tolist()\n",
        "    for dictionary in summaries:\n",
        "        facts = facts + dictionary['summary_text'] + '\\n'\n",
        "    prompt_text = prompt_text.format(\n",
        "        text_facts=facts,\n",
        "        user_question=input_text\n",
        "    )\n",
        "    response = llm_pipeline(prompt_text, do_sample=True)[0]\n",
        "    ### End ###\n",
        "\n",
        "    return response"
      ],
      "id": "cgH4EVCa3wh7"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6ygX1HvaFrqq"
      },
      "outputs": [],
      "source": [
        "input_text = \"is DALL-E2 uses a clip model inside?\"\n",
        "\n",
        "response = Unimodal_Question_Answering(input_text,k=1)"
      ],
      "id": "6ygX1HvaFrqq"
    },
    {
      "cell_type": "code",
      "source": [
        "response['generated_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "UZhIO75INq7A",
        "outputId": "808f66b7-0d65-4ad2-fc42-f3de185584e1"
      },
      "id": "UZhIO75INq7A",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ANSWER the QUESTION in conformity to on FACTS. \\n\\n    FACTS: \\n Since its release, CLIP has been used extensively to steer generative image models towards text prompts. Nichol et al. [35] showed classiﬁer-free guidance works more favorably than CLIP guidance for text conditional image generation. Zhou and Crowson [9] trained diffusion models conditioned on CLIP text embeddings, allowing for direct text-conditional imagegeneration.\\n. \\n\\n    QUESTION: is DALL-E2 uses a clip model inside? \\n\\n    ANSWER:  \\n    \\n    Based on the information provided, it appears that DALL-E2 does not use a clip model as an internal component. Instead, DALL-E2 is designed as a general-purpose framework for training and evaluating GAN architectures using real-world data sets and tasks. The framework includes features such as transfer learning, multi-task learning, and fine-tuning, which can be applied to different types of datasets and tasks without modifying the underlying architecture. Therefore, while DALL-E2 may have some similarities with CLIP-based approaches, it is fundamentally different from them in terms of its design and implementation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow9VAnRjSc6Y"
      },
      "source": [
        "# 2 - Multimodal RAG"
      ],
      "id": "Ow9VAnRjSc6Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf0B7mSrUDwg"
      },
      "source": [
        "In this section, we want to add another modality to our unimodal RAG. What happens if we can consider images as ground truth facts?\n",
        "\n",
        "We have stored all ground truth images. Thus, in this step, we should extract image embeddings for comparison with textual input embeddings"
      ],
      "id": "tf0B7mSrUDwg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aa7f52f-bf5c-4ba4-af72-b2ccba59a4cf"
      },
      "source": [
        "## 2.1 - Loading CLIP Model for Extracting Embeddings"
      ],
      "id": "0aa7f52f-bf5c-4ba4-af72-b2ccba59a4cf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9beLY0n6aq8"
      },
      "source": [
        "<font color='77CC99'> Load CLIP model for extracting textual and visial embeddings, then convert all input images to their corresponding vectors.\n",
        "\n",
        "[Huggingface Link](https://huggingface.co/docs/transformers/model_doc/clip) </font>\n"
      ],
      "id": "J9beLY0n6aq8"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kOSEU_Tryj4Z"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import AutoTokenizer, CLIPTextModelWithProjection\n",
        "from transformers import AutoProcessor, CLIPVisionModelWithProjection\n",
        "\n",
        "### To Do ###\n",
        "\n",
        "textual_clip_model = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "textual_clip_tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "visual_clip_model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "visual_clip_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "\n",
        "### End ###"
      ],
      "id": "kOSEU_Tryj4Z"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "v0Ckb62n7sK7"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "images_path = glob.glob('./images/*')\n",
        "\n",
        "### To Do ###\n",
        "\n",
        "images_embeddings = []\n",
        "\n",
        "for image_path in images_path:\n",
        "    im = Image.open(image_path)\n",
        "    inputs = visual_clip_processor(images=im, return_tensors='pt')\n",
        "    outputs = visual_clip_model(**inputs).last_hidden_state.detach().cpu().numpy()\n",
        "    images_embeddings.append(outputs)\n",
        "\n",
        "### End ###"
      ],
      "id": "v0Ckb62n7sK7"
    },
    {
      "cell_type": "code",
      "source": [
        "images_embeddings = torch.tensor(images_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIqGa2xxfcDc",
        "outputId": "267edb9e-7988-4537-e0ef-59df3e8adb47"
      },
      "id": "LIqGa2xxfcDc",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-567284ec3411>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  images_embeddings = torch.tensor(images_embeddings)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_embeddings = images_embeddings.squeeze()"
      ],
      "metadata": {
        "id": "hEQN_Kl7f6ET"
      },
      "id": "hEQN_Kl7f6ET",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_embeddings = torch.mean(images_embeddings, dim=1)"
      ],
      "metadata": {
        "id": "NIDEkAzkgbIi"
      },
      "id": "NIDEkAzkgbIi",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GAvQ1rHgYxE",
        "outputId": "0bab4d75-a1a3-46b2-b6bc-cee608e249de"
      },
      "id": "7GAvQ1rHgYxE",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([218, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBOqu26c886c"
      },
      "source": [
        "As we are using unimodsl LLM, we need to make image's information understandable for LLM. Hence, we extract textual information of imaged as \"Caption\" store them in \"captions\" list.\n",
        "\n",
        "<font color='77CC99'>Write the corresponding code.</font>\n",
        "\n",
        "\n",
        "[Image Captioning HF Model](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)"
      ],
      "id": "UBOqu26c886c"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XNMyGvP688hB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ac9988-036a-4abb-cfaf-e2dc5c3802fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "image_to_text = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "### To Do ###\n",
        "\n",
        "caption_list = []\n",
        "\n",
        "for image_path in images_path:\n",
        "    outputs = image_to_text(images=image_path)[0]['generated_text']\n",
        "    caption_list.append(outputs)\n",
        "\n",
        "\n",
        "### End ###"
      ],
      "id": "XNMyGvP688hB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJCMakba-zuG"
      },
      "source": [
        "Ok, now that we have every thing ready, we can code the Multimodal Semi-Structured RAG"
      ],
      "id": "bJCMakba-zuG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytZvBDF4-wJo"
      },
      "source": [
        "## 2.2 - Multimodal Semi-Structured RAG"
      ],
      "id": "ytZvBDF4-wJo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyYOZcmQBkCn"
      },
      "source": [
        "### Step 2.2.1: Most Similar Ground Truth Image Ectraction"
      ],
      "id": "GyYOZcmQBkCn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh1fWz_3BsHb"
      },
      "source": [
        "At the fist step, for any given input, we have to have evaluation functions to find the closest visual embedding vector to the input's textual vectors. We use the Cosine similarity for this operation.\n",
        "\n",
        "<font color='77CC99'>Write a function \"visual_embedding_similarity\" to convert input texts to clip embedding vector and then returns the similarity between the input text and any of the ground truth images.</font>\n",
        "\n",
        "Note: You can use \"get_similarity\" function that you have definced before."
      ],
      "id": "Oh1fWz_3BsHb"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "IWvYyz6nzHvR"
      },
      "outputs": [],
      "source": [
        "def get_embedding_similarity(input_text, images_embeddings, textual_clip_tokenizer, textual_clip_model):\n",
        "\n",
        "    ### To Do ###\n",
        "    inputs = textual_clip_tokenizer(input_text, return_tensors=\"pt\")\n",
        "    text_embeds = textual_clip_model(**inputs).last_hidden_state[0, -1].detach()\n",
        "    ### End ###\n",
        "\n",
        "    return get_similarity(images_embeddings[:, :512], text_embeds)"
      ],
      "id": "IWvYyz6nzHvR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCd0AKlJC4nu"
      },
      "source": [
        "<font color='77CC99'>Now, write a function that finds k most similar Text/Image to user's input.</font>"
      ],
      "id": "qCd0AKlJC4nu"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KIf47RTmC-_m"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "\n",
        "def multimodal_retrival(k,input_text,text_embeddings,text_elements,summarized_text_elements,\n",
        "                        text_emb_model,images_embeddings,caption_list,textual_clip_tokenizer ,textual_clip_model):\n",
        "\n",
        "    ### To Do ###\n",
        "    image_scores = get_embedding_similarity(input_text, images_embeddings, textual_clip_tokenizer, textual_clip_model)\n",
        "    ind = np.argsort(image_scores)[::-1][:k]\n",
        "    selected_image_elements = np.array(caption_list)[ind]\n",
        "\n",
        "    selected_text_elements = text_retrival(k, input_text, text_embeddings, text_elements, summarized_text_elements, text_emb_model)['selected_text_elements']\n",
        "    return {\"selected_image_elements\": selected_image_elements,\n",
        "          \"selected_text_elements\": selected_text_elements}\n",
        "\n",
        "    ### End ###"
      ],
      "id": "KIf47RTmC-_m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0pXU4CFGBBN"
      },
      "source": [
        "### Step 2.2.2: Use the core LLM and Combine them all"
      ],
      "id": "q0pXU4CFGBBN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hFtceV3GHu5"
      },
      "source": [
        "In this section, based on what we have done before(Loading LLM), we want to use what we have done in this section to write the Multimodal RAG. Do it as follows.\n",
        "\n",
        "<font color='77CC99'> Based on the new prompt which contains both textual ground truth facts and the caption of visual ground truth images, to write the \"Multimodal_Question_Answering\" function. This function should takes the user's textual question as input, then finds the most correlated textual and visual ground truth. Then gives them all to LLM via prompt.</font>"
      ],
      "id": "8hFtceV3GHu5"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JNrm8k0fGHdY"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "prompt_text = \"\"\"ANSWER the QUESTION in conformity to on FACTS. \\n\n",
        "FACTS: \\n {text_facts} \\n {image_facts}. \\n\n",
        "QUESTION: {user_question} \\n\n",
        "ANSWER:  \"\"\""
      ],
      "id": "JNrm8k0fGHdY"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Ma9wLfimHhtW"
      },
      "outputs": [],
      "source": [
        "def Multimodal_Question_Answering(input_text,k=1):\n",
        "    prompt_text = \"\"\"ANSWER the QUESTION in conformity to on FACTS. \\n\n",
        "    FACTS: \\n {text_facts} \\n {image_facts}. \\n\n",
        "    QUESTION: {user_question} \\n\n",
        "    ANSWER:  \"\"\"\n",
        "    ### To Do ###\n",
        "    summaries = multimodal_retrival(k,input_text,text_embeddings,text_elements,summarized_text_elements,\n",
        "                        text_emb_model,images_embeddings,caption_list,textual_clip_tokenizer ,textual_clip_model)\n",
        "    text_facts = \"\"\n",
        "    image_facts = \"\"\n",
        "\n",
        "\n",
        "    for dictionary in list(summaries['selected_image_elements']):\n",
        "        image_facts = image_facts + dictionary + '\\n'\n",
        "\n",
        "    for dictionary in list(summaries['selected_text_elements']):\n",
        "        text_facts = text_facts + dictionary['summary_text'] + '\\n'\n",
        "\n",
        "    prompt_text = prompt_text.format(\n",
        "        text_facts=text_facts,\n",
        "        image_facts=image_facts,\n",
        "        user_question=input_text\n",
        "    )\n",
        "    response = llm_pipeline(prompt_text, do_sample=True)[0]\n",
        "    ### End ###\n",
        "\n",
        "    return response"
      ],
      "id": "Ma9wLfimHhtW"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ckWn-6HXH1cN"
      },
      "outputs": [],
      "source": [
        "input_text = \"is DALL-E2 uses a clip model inside?\"\n",
        "\n",
        "response = Multimodal_Question_Answering(input_text,k=1)"
      ],
      "id": "ckWn-6HXH1cN"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "yYXhGZExIBjk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "de3d5c24-4bad-4b3d-db84-ce190281c7ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ANSWER the QUESTION in conformity to on FACTS. \\n\\n    FACTS: \\n Since its release, CLIP has been used extensively to steer generative image models towards text prompts. Nichol et al. [35] showed classiﬁer-free guidance works more favorably than CLIP guidance for text conditional image generation. Zhou and Crowson [9] trained diffusion models conditioned on CLIP text embeddings, allowing for direct text-conditional imagegeneration.\\n \\n a bowl of fruit on a table \\n. \\n\\n    QUESTION: is DALL-E2 uses a clip model inside? \\n\\n    ANSWER:  \\n    \\n    Yes, it does use a Clip model. According to the paper \"DALL-E2: Text-conditioned Image Generation with Improved Quality\" by Liu et al., they state that \"To achieve robustness, we propose a new architecture called DALL-E2, which consists of two parts: a Clip generator that encodes context information into the image space using a language model, and a diffraction network that computes an embedding of the image.\" \\n    \\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "response['generated_text']"
      ],
      "id": "yYXhGZExIBjk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBAo_j4aPEBa"
      },
      "source": [
        "<font color='77CC99'>The Answer to the input question is \"Yes\" or \"No\". What are your Semi-structured models' answers? (Both Unimodal and Multimodal). Are they right or not?</font>"
      ],
      "id": "uBAo_j4aPEBa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9xbW4lbPqKs"
      },
      "source": [
        "<font color='CC7799'>Your Answer:</font> Interestingly the unimodal model's answer is incorrect but the multimodal model was able to give the correct answer. This could be due to some information being present in images of the paper and not in the text."
      ],
      "id": "z9xbW4lbPqKs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Jb5GXVPyL9"
      },
      "source": [
        "...."
      ],
      "id": "f5Jb5GXVPyL9"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "e6cEOI6IL5TL",
        "9n9HKjA2an9U"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fcaf954ee4464893ac2f8070d29e8439": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99fd43a0605943cfa67dd1bc353ffe91",
              "IPY_MODEL_24a6dfd243624575b76fb91a3e9749a4",
              "IPY_MODEL_d895d677f2f846db8d7570b48115a320"
            ],
            "layout": "IPY_MODEL_e76ae40090884615a964c7ca40593c44"
          }
        },
        "99fd43a0605943cfa67dd1bc353ffe91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_573db3f4dd374b59bf7a4c24aba5f569",
            "placeholder": "​",
            "style": "IPY_MODEL_4d15859d1dea4c199cb9c768f034128e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "24a6dfd243624575b76fb91a3e9749a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df76991414e24a4bac7a8101d45586e4",
            "max": 14,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d4fbf7741f84846b027f8e45ce1e2df",
            "value": 14
          }
        },
        "d895d677f2f846db8d7570b48115a320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84902c91fb87451b89efc9fd16152053",
            "placeholder": "​",
            "style": "IPY_MODEL_9b8cf41e469142b0b8afa718f16fddf3",
            "value": " 14/14 [02:49&lt;00:00, 11.87s/it]"
          }
        },
        "e76ae40090884615a964c7ca40593c44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "573db3f4dd374b59bf7a4c24aba5f569": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d15859d1dea4c199cb9c768f034128e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df76991414e24a4bac7a8101d45586e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d4fbf7741f84846b027f8e45ce1e2df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84902c91fb87451b89efc9fd16152053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b8cf41e469142b0b8afa718f16fddf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}